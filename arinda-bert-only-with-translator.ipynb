{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install deep_translator\n!pip install transformers\n\nimport numpy as np\nimport pandas as pd\n\nimport seaborn as sns\nfrom deep_translator import GoogleTranslator\nimport tensorflow as tf\nimport transformers\nfrom sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.environ[\"WANDB_API_KEY\"] = \"0\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"try:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nexcept ValueError:\n    strategy = tf.distribute.get_strategy() # for CPU and single GPU\n    print('Number of replicas:', strategy.num_replicas_in_sync)\n    \ntf.config.experimental_connect_to_cluster(tpu)\ntf.tpu.experimental.initialize_tpu_system(tpu)\ntpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"../input/contradictory-my-dear-watson/train.csv\")\ntest = pd.read_csv(\"../input/contradictory-my-dear-watson/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train.reset_index()\ntest = test.reset_index()\ntrain.columns\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['lang_abv'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Translate To English using Google Translator"},{"metadata":{"trusted":true},"cell_type":"code","source":"def trans_to_eng(row):\n    premise = row['premise']\n    hypothesis = row['hypothesis']\n    \n    nmber = row['index']\n    source = row['lang_abv']\n    target = 'en'\n    \n    if source != 'en':\n        en_return_premise = GoogleTranslator(source=source, target=target).translate(premise)\n        en_return_hypothesis = GoogleTranslator(source=source, target=target).translate(hypothesis)\n    else:\n        en_return_premise = premise\n        en_return_hypothesis = hypothesis\n       \n    #print(en_return)\n    print(nmber)\n    return en_return_premise, en_return_hypothesis\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['premise_en'], train['hypothesis_en'] = zip(*train.apply(lambda x: trans_to_eng(x), axis = 1 ))\n\n#train['premise_en'] = np.where(train['lang_abv'] != 'en',GoogleTranslator(source=train['lang_abv'], target='en').translate(train['premise']),train['premise'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test['premise_en'], test['hypothesis_en'] = zip(*test.apply(lambda x: trans_to_eng(x), axis = 1 ))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Sample Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"premise: {train.loc[4, 'premise_en']}\")\nprint(f\"hypothesis: {train.loc[4, 'hypothesis_en']}\")\nprint(f\"label: {train.loc[4, 'label']}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Configurarion for the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"max_length = 100  # Maximum length of input sentence to the model.\nbatch_size = 16\nepochs = 10","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Tokenization of the input in the shape that BERT requires"},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = transformers.BertTokenizer.from_pretrained(\"bert-base-uncased\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"encoding_train = tokenizer(text=list(train.premise_en.values),\n                    text_pair=list(train.hypothesis_en.values),\n                    add_special_tokens=True,\n                    max_length=max_length,\n                    truncation=True,\n                    padding=True,\n                    return_attention_mask=True,\n                    return_token_type_ids=True,\n                    return_tensors='tf'\n                    )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nvalid_size = int(len(train)*0.33)\n\nencoded_train_data = tf.data.Dataset.from_tensor_slices((encoding_train.data, train.label.values))\nvalidation_dataset = (encoded_train_data.take(valid_size).batch(batch_size))\ntrain_dataset = (encoded_train_data.skip(valid_size).batch(batch_size))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Initialise the Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"MODEL_NAME = 'bert-base-uncased'\nwith tpu_strategy.scope():\n    transformer = transformers.TFAutoModel.from_pretrained(MODEL_NAME)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def base_bert_model():\n\n    input_ids = tf.keras.Input(shape=(max_length,),name='input_ids', dtype='int32')\n    attention_mask = tf.keras.Input(shape=(max_length,),name='attention_mask', dtype='int32')\n    token_type_ids = tf.keras.Input(shape=(max_length,),name='token_type_ids', dtype='int32')\n\n    bert_transformed = transformer((input_ids, attention_mask, token_type_ids))[0]\n    output_1 = tf.keras.layers.Dense(300, activation='relu')(bert_transformed[:,0,:])\n    output_2 = tf.keras.layers.Dense(100, activation='relu')(output_1)\n    output = tf.keras.layers.Dense(3, activation='softmax', name='output_layer')(output_2)\n    \n    model = tf.keras.Model(inputs=(input_ids, attention_mask, token_type_ids), outputs=output)\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with tpu_strategy.scope():\n    model = base_bert_model()\n    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5)\n    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n    model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n    model.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Run Model "},{"metadata":{"trusted":true},"cell_type":"code","source":"loss_reduction = tf.keras.callbacks.ReduceLROnPlateau(\n    monitor='val_loss',\n    factor=0.3,\n    patience= 1,\n    min_lr=5e-7\n)\n\nhistory = model.fit(\n    train_dataset,\n    epochs = epochs,\n    verbose = 2,\n    batch_size = batch_size,\n    callbacks=[loss_reduction],\n    validation_data=validation_dataset\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Prepare Submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"encoding_test = tokenizer(text=list(test.premise_en.values),\n                    text_pair=list(test.hypothesis_en.values),\n                    add_special_tokens=True,\n                    max_length=max_length,\n                    truncation=True,\n                    padding=True,\n                    return_attention_mask=True,\n                    return_token_type_ids=True,\n                    return_tensors='tf'\n                    )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_submission = model.predict(encoding_test.data, batch_size=128, verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_pred_labels = np.argmax(pred_submission, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame()\nsubmission['prediction'] = test_pred_labels","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv(\"submission.csv\", index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}